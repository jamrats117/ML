

---

## üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: ‚Äú‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?‚Äù

| ‡∏Å‡∏£‡∏ì‡∏µ                                                          | ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏°?          | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•                                       |
| ------------------------------------------------------------- | ------------------------ | -------------------------------------------- |
| ‡πÉ‡∏ä‡πâ ANN, GBR, CatBoost ‡∏à‡∏≤‡∏Å library ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ (‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡∏ô‡∏´‡∏•‡∏±‡∏Å) | ‚ùå ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô              | ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô + reference ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ |
| ‡∏™‡∏≠‡∏ô‡∏Å‡∏•‡πÑ‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏≤‡∏¢ ML                       | ‚úÖ ‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ 1‚Äì2 ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢ ‡πÜ | ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á perceptron ‡∏´‡∏£‡∏∑‡∏≠ loss function  |
| ‡∏õ‡∏£‡∏±‡∏ö architecture ‡πÄ‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô activation ‡πÉ‡∏´‡∏°‡πà, optimizer ‡πÉ‡∏´‡∏°‡πà    | ‚úÖ ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà                 | ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á novelty                            |
| ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á journal ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô theory-heavy ‡πÄ‡∏ä‡πà‡∏ô applied mathematics | ‚úÖ ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà                 | ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠                |

---

## ‚úçÔ∏è ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö‡∏û‡∏≠‡∏î‡∏µ ‡πÜ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)

‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ANN ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì output node:

> The output $\hat{y}$ of a fully connected feedforward ANN with one hidden layer can be expressed as:

$$
\hat{y} = f\left(\sum_{j=1}^{n} w_j^{(2)} \cdot \phi\left(\sum_{i=1}^{m} w_{ij}^{(1)} x_i + b_j^{(1)}\right) + b^{(2)}\right)
$$

Where:

* $x_i$: input features
* $w_{ij}^{(1)}$, $w_j^{(2)}$: weights of hidden and output layers
* $b_j^{(1)}$, $b^{(2)}$: biases
* $\phi$: activation function (e.g., ReLU)
* $f$: output activation (e.g., linear or sigmoid)

üëÜ ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ **‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ** ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô


---

## ‚úÖ 1. **Artificial Neural Network (ANN)**

(‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏Ç‡∏≠‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)

$$
\hat{y} = f\left(\sum_{j=1}^{n} w_j^{(2)} \cdot \phi\left(\sum_{i=1}^{m} w_{ij}^{(1)} x_i + b_j^{(1)}\right) + b^{(2)}\right)
$$

Where:

* $x_i$: input features
* $w^{(1)}_{ij}$, $w^{(2)}_j$: weights of hidden and output layers
* $b^{(1)}_j$, $b^{(2)}$: biases
* $\phi$: activation function in hidden layer
* $f$: output activation function (e.g., linear for regression)
* $\hat{y}$: predicted output

---

## ‚úÖ 2. **Gradient Boosting Regression (GBR)**

GBR builds an ensemble model sequentially by fitting each learner to the residuals of the previous ensemble. The model prediction after $M$ iterations is:

$$
\hat{y}(x) = \sum_{m=1}^{M} \gamma_m h_m(x)
$$

Where:

* $h_m(x)$: the weak learner (usually a decision tree) at iteration $m$
* $\gamma_m$: learning rate or step size
* $\hat{y}(x)$: final prediction after $M$ iterations

The model is trained to minimize a differentiable loss function $L(y, \hat{y})$, and at each stage:

$$
r_{i}^{(m)} = - \left[ \frac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i} \right]_{\hat{y}_i = \hat{y}^{(m-1)}_i}
$$

Where $r_{i}^{(m)}$ is the pseudo-residual for sample $i$ at iteration $m$.

---

## ‚úÖ 3. **CatBoost**

CatBoost is also a gradient boosting method, but it improves upon conventional GBR with innovations like **ordered boosting** and **native categorical handling**. Its overall prediction form is:

$$
\hat{y}(x) = \sum_{m=1}^{M} h_m(x; \Theta_m)
$$

Where:

* $h_m(x; \Theta_m)$: tree at iteration $m$ with learned parameters $\Theta_m$
* The training uses **ordered boosting**, which ensures that each prediction is made without information leakage by constructing separate statistics per boosting step.

Additionally, CatBoost applies **target statistics (TS)** for encoding categorical features:

$$
\text{TS}_j = \frac{\sum_{i \in \mathcal{B}_j} y_i + a}{|\mathcal{B}_j| + b}
$$

Where:

* $\mathcal{B}_j$: set of samples with category $j$
* $a$, $b$: smoothing parameters to reduce overfitting

Although CatBoost uses the same loss minimization principle as GBR, its implementation is distinguished by handling categorical features natively and reducing prediction shift.

---

## ‚ú® ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ô Section 2.2

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏£‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏ä‡πà‡∏ô:

> The final prediction in GBR is an additive combination of sequentially fitted decision trees:
>
> $$
> \hat{y}(x) = \sum_{m=1}^{M} \gamma_m h_m(x)
> $$
>
> where $h_m(x)$ denotes the base learner at iteration $m$, and $\gamma_m$ is the learning rate.

‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö boxed equation ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö

