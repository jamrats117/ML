

---

## üîç ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÅ‡∏ö‡∏ö‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î: ‚Äú‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà?‚Äù

| ‡∏Å‡∏£‡∏ì‡∏µ                                                          | ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÑ‡∏´‡∏°?          | ‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•                                       |
| ------------------------------------------------------------- | ------------------------ | -------------------------------------------- |
| ‡πÉ‡∏ä‡πâ ANN, GBR, CatBoost ‡∏à‡∏≤‡∏Å library ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏£‡∏π‡∏õ (‡πÑ‡∏°‡πà‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡∏ô‡∏´‡∏•‡∏±‡∏Å) | ‚ùå ‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô              | ‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô + reference ‡πÄ‡∏û‡∏µ‡∏¢‡∏á‡∏û‡∏≠ |
| ‡∏™‡∏≠‡∏ô‡∏Å‡∏•‡πÑ‡∏Å‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏™‡∏≤‡∏¢ ML                       | ‚úÖ ‡πÉ‡∏™‡πà‡πÑ‡∏î‡πâ 1‚Äì2 ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏á‡πà‡∏≤‡∏¢ ‡πÜ | ‡πÄ‡∏ä‡πà‡∏ô ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á perceptron ‡∏´‡∏£‡∏∑‡∏≠ loss function  |
| ‡∏õ‡∏£‡∏±‡∏ö architecture ‡πÄ‡∏≠‡∏á ‡πÄ‡∏ä‡πà‡∏ô activation ‡πÉ‡∏´‡∏°‡πà, optimizer ‡πÉ‡∏´‡∏°‡πà    | ‚úÖ ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà                 | ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á novelty                            |
| ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏•‡∏á journal ‡∏ó‡∏µ‡πà‡πÄ‡∏ô‡πâ‡∏ô theory-heavy ‡πÄ‡∏ä‡πà‡∏ô applied mathematics | ‚úÖ ‡∏Ñ‡∏ß‡∏£‡πÉ‡∏™‡πà                 | ‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏ñ‡∏∑‡∏≠                |

---

## ‚úçÔ∏è ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏ö‡∏ö‡∏û‡∏≠‡∏î‡∏µ ‡πÜ (‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£)

‡∏ñ‡πâ‡∏≤‡∏Ñ‡∏∏‡∏ì‡∏≠‡∏¢‡∏≤‡∏Å‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ANN ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì output node:

> The output $\hat{y}$ of a fully connected feedforward ANN with one hidden layer can be expressed as:

$$
\hat{y} = f\left(\sum_{j=1}^{n} w_j^{(2)} \cdot \phi\left(\sum_{i=1}^{m} w_{ij}^{(1)} x_i + b_j^{(1)}\right) + b^{(2)}\right)
$$

Where:

* $x_i$: input features
* $w_{ij}^{(1)}$, $w_j^{(2)}$: weights of hidden and output layers
* $b_j^{(1)}$, $b^{(2)}$: biases
* $\phi$: activation function (e.g., ReLU)
* $f$: output activation (e.g., linear or sigmoid)

üëÜ ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ **‡πÑ‡∏°‡πà‡πÄ‡∏¢‡∏≠‡∏∞‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ** ‡πÅ‡∏•‡∏∞‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏™‡∏≤‡∏¢‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏´‡∏•‡∏±‡∏Å‡∏Å‡∏≤‡∏£‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏´‡∏•‡∏±‡∏á‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô


---

## ‚úÖ 1. **Artificial Neural Network (ANN)**

(‡∏Ñ‡∏∏‡∏ì‡∏°‡∏µ‡πÅ‡∏•‡πâ‡∏ß ‡πÅ‡∏ï‡πà‡∏Ç‡∏≠‡πÉ‡∏™‡πà‡πÑ‡∏ß‡πâ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á)

$$
\hat{y} = f\left(\sum_{j=1}^{n} w_j^{(2)} \cdot \phi\left(\sum_{i=1}^{m} w_{ij}^{(1)} x_i + b_j^{(1)}\right) + b^{(2)}\right)
$$

Where:

* $x_i$: input features
* $w^{(1)}_{ij}$, $w^{(2)}_j$: weights of hidden and output layers
* $b^{(1)}_j$, $b^{(2)}$: biases
* $\phi$: activation function in hidden layer
* $f$: output activation function (e.g., linear for regression)
* $\hat{y}$: predicted output

---

## ‚úÖ 2. **Gradient Boosting Regression (GBR)**

GBR builds an ensemble model sequentially by fitting each learner to the residuals of the previous ensemble. The model prediction after $M$ iterations is:

$$
\hat{y}(x) = \sum_{m=1}^{M} \gamma_m h_m(x)
$$

Where:

* $h_m(x)$: the weak learner (usually a decision tree) at iteration $m$
* $\gamma_m$: learning rate or step size
* $\hat{y}(x)$: final prediction after $M$ iterations

The model is trained to minimize a differentiable loss function $L(y, \hat{y})$, and at each stage:

$$
r_{i}^{(m)} = - \left[ \frac{\partial L(y_i, \hat{y}_i)}{\partial \hat{y}_i} \right]_{\hat{y}_i = \hat{y}^{(m-1)}_i}
$$

Where $r_{i}^{(m)}$ is the pseudo-residual for sample $i$ at iteration $m$.

---

## ‚úÖ 3. **CatBoost**

CatBoost is also a gradient boosting method, but it improves upon conventional GBR with innovations like **ordered boosting** and **native categorical handling**. Its overall prediction form is:

$$
\hat{y}(x) = \sum_{m=1}^{M} h_m(x; \Theta_m)
$$

Where:

* $h_m(x; \Theta_m)$: tree at iteration $m$ with learned parameters $\Theta_m$
* The training uses **ordered boosting**, which ensures that each prediction is made without information leakage by constructing separate statistics per boosting step.

Additionally, CatBoost applies **target statistics (TS)** for encoding categorical features:

$$
\text{TS}_j = \frac{\sum_{i \in \mathcal{B}_j} y_i + a}{|\mathcal{B}_j| + b}
$$

Where:

* $\mathcal{B}_j$: set of samples with category $j$
* $a$, $b$: smoothing parameters to reduce overfitting

Although CatBoost uses the same loss minimization principle as GBR, its implementation is distinguished by handling categorical features natively and reducing prediction shift.

---

## ‚ú® ‡πÄ‡∏Ñ‡∏•‡πá‡∏î‡∏•‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÉ‡∏™‡πà‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ô Section 2.2

‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏´‡∏•‡πà‡∏≤‡∏ô‡∏µ‡πâ‡πÅ‡∏ó‡∏£‡∏Å‡πÉ‡∏ô‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÄ‡∏ä‡πà‡∏ô:

> The final prediction in GBR is an additive combination of sequentially fitted decision trees:
>
> $$
> \hat{y}(x) = \sum_{m=1}^{M} \gamma_m h_m(x)
> $$
>
> where $h_m(x)$ denotes the base learner at iteration $m$, and $\gamma_m$ is the learning rate.

‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏ä‡πâ‡πÅ‡∏ö‡∏ö boxed equation ‡πÅ‡∏•‡πâ‡∏ß‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡πâ‡∏ô ‡πÜ ‡∏Å‡πá‡πÑ‡∏î‡πâ ‡∏Ç‡∏∂‡πâ‡∏ô‡∏Å‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡πÄ‡∏•‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏≤‡∏£‡∏™‡∏≤‡∏£‡∏Ñ‡∏£‡∏±‡∏ö

‡πÉ‡∏ä‡πà‡∏Ñ‡∏£‡∏±‡∏ö ‚úÖ
‡∏™‡∏°‡∏Å‡∏≤‡∏£‡πÉ‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô **Shapley value formula** ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì SHAP (SHapley Additive exPlanations) ‡πÇ‡∏î‡∏¢‡∏°‡∏≤‡∏à‡∏≤‡∏Å **‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÄ‡∏Å‡∏° (cooperative game theory)**

---

### üîç ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡πÉ‡∏ô‡∏™‡∏°‡∏Å‡∏≤‡∏£:

$$
\phi_i = \sum_{S \subseteq F \setminus \{i\}} \frac{|S|! (|F| - |S| - 1)!}{|F|!} \left[ f_{S \cup \{i\}}(x_{S \cup \{i\}}) - f_S(x_S) \right]
$$

| ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå  | ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢                                      |    |                                  |   |   |         |   |     |                                     |
| ---------- | --------------------------------------------- | -- | -------------------------------- | - | - | ------- | - | --- | ----------------------------------- |
| $\phi_i$   | ‡∏Ñ‡πà‡∏≤ **SHAP value** ‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà $i$          |    |                                  |   |   |         |   |     |                                     |
| $F$        | ‡πÄ‡∏ã‡∏ï‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î                          |    |                                  |   |   |         |   |     |                                     |
| $S$        | subset ‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° $i$                |    |                                  |   |   |         |   |     |                                     |
| $f_S(x_S)$ | ‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ feature subset $S$ |    |                                  |   |   |         |   |     |                                     |
| (          | F                                             | !) | factorial ‡∏Ç‡∏≠‡∏á‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î |   |   |         |   |     |                                     |
| (\frac{    | S                                             | !( | F                                | - | S | - 1)!}{ | F | !}) | ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ô‡πà‡∏≤‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏≠‡∏á subset ‡∏ô‡∏±‡πâ‡∏ô |

---

### üß† ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏ä‡∏¥‡∏á‡πÅ‡∏ô‡∏ß‡∏Ñ‡∏¥‡∏î:

* ‡πÄ‡∏£‡∏≤‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå $i$ ‡πÇ‡∏î‡∏¢‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏ß‡πà‡∏≤ model ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Ñ‡πà‡∏≤‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡πÅ‡∏Ñ‡πà‡πÑ‡∏´‡∏ô **‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà/‡πÑ‡∏°‡πà‡πÉ‡∏™‡πà** ‡∏ü‡∏µ‡πÄ‡∏à‡∏≠‡∏£‡πå‡∏ô‡∏µ‡πâ
* ‡πÅ‡∏•‡πâ‡∏ß‡∏ô‡∏≥‡∏ú‡∏•‡∏ô‡∏±‡πâ‡∏ô **‡∏ñ‡πà‡∏ß‡∏á‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å** ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å subset $S$ ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏£‡∏ß‡∏° $i$ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà ‚Äú‡∏¢‡∏∏‡∏ï‡∏¥‡∏ò‡∏£‡∏£‡∏°‚Äù ‡∏ï‡∏≤‡∏°‡∏ó‡∏§‡∏©‡∏é‡∏µ‡πÄ‡∏Å‡∏°

---

‡∏´‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏™‡∏°‡∏Å‡∏≤‡∏£‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î** ‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏∞‡πÉ‡∏´‡πâ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡πÉ‡∏ô‡∏™‡πÑ‡∏ï‡∏•‡πå‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô/‡∏™‡πÑ‡∏•‡∏î‡πå ‡∏Å‡πá‡πÅ‡∏à‡πâ‡∏á‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏±‡∏ö ‚úÖ
